{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing a corpus of books using natural language processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Books about Istanbul are scraped and is then further processed to get sentences based on the object list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dVy1XFYbS7Pe"
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import nltk.corpus\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6lL9wsXaTKQa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import Word2Vec as w2v\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wvQtTHumTLny"
   },
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mWGlfQQ_TMr6",
    "outputId": "157a9d57-2b31-4fef-dbca-b615cf537aac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ijJ9YqenTOOq"
   },
   "outputs": [],
   "source": [
    "folder_path = \"/content/drive/MyDrive/Istanbul_Books\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qtIlr8NQTUVR"
   },
   "outputs": [],
   "source": [
    "def break_sentences(text):\n",
    "    # replace newline characters with spaces\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "\n",
    "    # split the text into sentences based on .?! characters\n",
    "    sentences = re.split(r'[.?!]+', text)\n",
    "\n",
    "    # remove empty strings and leading/trailing spaces from sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JifL4Bp1TUva"
   },
   "outputs": [],
   "source": [
    "def clean_string(s):\n",
    "    # remove any sequence of more than two spaces with a single space\n",
    "    s = re.sub(r' {2,}', ' ', s)\n",
    "\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2COAezEITW6h",
    "outputId": "7ccbf406-470f-4009-ba8d-972c7ba2cc74"
   },
   "outputs": [],
   "source": [
    "collection = []\n",
    "read_books = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        # read the file contents\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "        \n",
    "        # break the text into sentences\n",
    "        sentences = break_sentences(text)\n",
    "        \n",
    "        # add the sentences to the collection list\n",
    "        collection += sentences\n",
    "        \n",
    "        # add the file name to the read_books list\n",
    "        read_books.append(filename)\n",
    "        \n",
    "# print the names of the files that were read\n",
    "print(f\"Read {len(read_books)} files:\")\n",
    "\n",
    "# print the sentences\n",
    "for i, sentence in enumerate(collection):\n",
    "    print(f\"Index {i}: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QyDf6aL4TYip",
    "outputId": "29424f47-9332-4465-810f-fe8fab148603"
   },
   "outputs": [],
   "source": [
    "final_collection = []\n",
    "\n",
    "for sentence in collection:\n",
    "    cleaned_sentence = clean_string(sentence)\n",
    "    final_collection.append(cleaned_sentence)\n",
    "\n",
    "for i, sentence in enumerate(final_collection):\n",
    "    print(f\"Index {i}: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uJ-9VWK1TbVJ",
    "outputId": "c239300e-7393-402c-adb7-e1b1859d53b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239739"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MQ7e28WJTeqi",
    "outputId": "f5cb7a1e-4647-4b16-c287-1dc938a6b512"
   },
   "outputs": [],
   "source": [
    "stoplist = set(\"and the or but nor yet so been now will are were would should did dont ever with was had have has\".split(' '))\n",
    "\n",
    "texts = [[word.replace(\".\",\"\").replace(\",\",\"\").replace(\"'\",\"\").replace(\":\",\"\") for word in document.lower().split()] \n",
    "         for document in final_collection]\n",
    "\n",
    "texts = [[word for word in text if (word not in stoplist and len(word)>2)] \n",
    "         for text in texts]\n",
    "\n",
    "to_delete = []\n",
    "for i in range(len(texts)):\n",
    "    t = texts[i]\n",
    "    test = [w for w in t if w.isalpha()]\n",
    "    if len(test) < 5:\n",
    "        to_delete.append(i)\n",
    "    else:\n",
    "        texts[i] = test\n",
    "\n",
    "for i in sorted(to_delete, reverse = True):\n",
    "    del texts[i]\n",
    "    del final_collection[i]\n",
    "        \n",
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XEiqe99UTfgR",
    "outputId": "225cdb66-09ba-44f0-fd7f-8f58c973ee8c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166551"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eXIFdPYMTjf5",
    "outputId": "74fffdd1-144b-4d81-f56e-39dd9c13f303"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<43345 unique tokens: ['architecture', 'art', 'certain', 'city', 'culture']...>\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K81ixbQ-TkcU",
    "outputId": "8d32a569-f32a-447b-d824-eaa5cf282972"
   },
   "outputs": [],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ugkR2oj1TmZi"
   },
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Jdzi9USeTn5B"
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "mM13mlFRTpaJ"
   },
   "outputs": [],
   "source": [
    "from gensim.similarities import Similarity\n",
    "from gensim.test.utils import get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bjI6idXJTqvp"
   },
   "outputs": [],
   "source": [
    "index_tmpfile = get_tmpfile(\"index\")\n",
    "\n",
    "index = Similarity(index_tmpfile, bow_corpus, num_features = len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "uckbz9xGTsFi"
   },
   "outputs": [],
   "source": [
    "query_document = \"balcony\".lower().split()\n",
    "query_bow = dictionary.doc2bow(query_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "UMjtLWZKTtSa"
   },
   "outputs": [],
   "source": [
    "sims = index[query_bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PIAS98G9Tua7",
    "outputId": "0ca59e0f-e081-462d-f2e8-325b75829c14"
   },
   "outputs": [],
   "source": [
    "for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "    print(document_number, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "fOK4iDzkTvuC"
   },
   "outputs": [],
   "source": [
    "top_scores = sorted(enumerate(sims), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bhMVcTuTwqL",
    "outputId": "2f2254fb-4f52-4b0e-a42d-3179ff0ae586"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: Nalan leaned over the balcony railing\n",
      "Sentence 2: ‘You can’t stand on the balcony and throw things at strangers\n",
      "Sentence 3: Women often use a screened-off area or a balcony\n",
      "Sentence 4: The views from its upper-storey balcony are stunning\n",
      "Sentence 5: “Come out here,” Joy calls from the balcony\n",
      "Sentence 6: Macit went out onto the balcony\n",
      "Sentence 7: No, they did not swing from the balcony\n",
      "Sentence 8: No, they did not hang off the balcony\n",
      "Sentence 9: Flat Number 7: Me Ants raided my balcony today – or perhaps it was just today that I noticed ants had raided my balcony\n",
      "Sentence 10: The dried aubergines were hanging from the kitchen balcony\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_sentences = [final_collection[idx] for idx, score in top_scores]\n",
    "\n",
    "# combine the top paragraphs and their corresponding books into a single string\n",
    "output = \"\"\n",
    "for i in range(len(top_sentences)):\n",
    "    output += f\"Sentence {i+1}: {top_sentences[i]}\\n\"\n",
    "        \n",
    "# write the output to a file named \"top_paragraphs.txt\"\n",
    "with open(\"./balcony.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(output)\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
